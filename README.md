<br>
<p align="center">
<h1 align="center"><strong>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</strong></h1>
  <p align="center">
    <a href='https://scholar.google.com/citations?user=MbzyV9YAAAAJ&hl=en' target='_blank'>Zhijun PanÂ¹*</a>&emsp;
    <a href='https://toytiny.github.io/' target='_blank'>Fangqiang DingÂ²*</a>&emsp;
    <a href='https://www.linkedin.com/in/hantao-zhong/' target='_blank'>Hantao ZhongÂ³*</a>&emsp;
    <a href='https://christopherlu.github.io/' target='_blank'>Chris Xiaoxuan Luâ´
    </a>&emsp;
    <br>
    *Equal Contribution
    <br>
    Royal College of ArtÂ¹&emsp;University of EdinburghÂ²&emsp;University of CambridgeÂ³&emsp;
    University College Londonâ´
  </p>
</p>

<div id="top" align="center">

[![arXiv](https://img.shields.io/badge/arXiv-2309.09737-b31b1b.svg)](https://arxiv.org/abs/2309.09737)
[![](https://img.shields.io/youtube/views/IxfyCWyNhfw?label=Demo&style=flat)](https://www.youtube.com/watch?v=IxfyCWyNhfw&feature=youtu.be)
[![GitHub license](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/LJacksonPan/RaTrack/blob/master/LICENSE)

</div>

<p align="center">
<img src='./doc/ratrack_pipeline.png' width="840">
</p>



## ğŸ”¥ News
 - [2024-01-29] Our paper is accepted by [ICRA 2024](https://2024.ieee-icra.org/) ğŸ‰.
 - [2024-01-29] Our paper can be seen here ğŸ‘‰ [arXiv](https://arxiv.org/abs/2309.09737)
 - [2024-03-13] We further improve the overall performance. Please check [Evaluation](#evaluation).
 - [2024-03-13] Our paper demo video can be seen here ğŸ‘‰[video](https://youtu.be/IxfyCWyNhfw)
## ğŸ”— Citation
If you find our work useful in your research, please consider citing:


```bibtex
@article{pan2023moving,
  title={Moving Object Detection and Tracking with 4D Radar Point Cloud},
  author={Pan, Zhijun and Ding, Fangqiang and Zhong, Hantao and Lu, Chris Xiaoxuan},
  journal={arXiv preprint arXiv:2309.09737},
  year={2023}
}
```

## ğŸ“Š Qualitative results
Here are some GIFs to show our qualitative results on tracking. For more qualitative results, please refer to our [demo video](#demo-video).

<p align="center">
<img src='./doc/ratrack_gif1_slow.gif' width="840">
</p>

<p align="center">
<img src='./doc/ratrack_gif2_slow.gif' width="840">
</p>

## âœ… Dataset Preparation

First, please request and download the View of Delft (VoD) dataset from the [official website](https://tudelft-iv.github.io/view-of-delft-dataset/). Unzip into the folder you prefer.

Please also obtain the tracking annotation from [VoD Github](https://github.com/tudelft-iv/view-of-delft-dataset/blob/main/docs/ANNOTATION.md). Unzip all the `.txt` tracking annotation files into the path: `PATH_TO_VOD_DATASET/view_of_delft_PUBLIC/lidar/training/label_2_tracking/`

The folder structure should look like this:

```
view_of_delft_PUBLIC/
â”œâ”€â”€ lidar
â”‚Â Â  â”œâ”€â”€ ImageSets
â”‚Â Â  â”œâ”€â”€ testing
â”‚Â Â  â””â”€â”€ training
â”‚Â Â      â”œâ”€â”€ calib
â”‚Â Â      â”œâ”€â”€ image_2
â”‚Â Â      â”œâ”€â”€ label_2
â”‚Â Â          â”œâ”€â”€ 00000.txt
â”‚Â Â          â”œâ”€â”€ 00001.txt
â”‚Â Â          â”œâ”€â”€ ...
â”‚Â Â      â”œâ”€â”€ label_2_tracking
â”‚Â Â          â”œâ”€â”€ 00000.txt
â”‚Â Â          â”œâ”€â”€ 00001.txt
â”‚Â Â          â”œâ”€â”€ ...
â”‚Â Â      â”œâ”€â”€ pose
â”‚Â Â      â””â”€â”€ velodyne
â”œâ”€â”€ radar
â”‚Â Â  â”œâ”€â”€ testing
â”‚Â Â  â””â”€â”€ training
â”‚Â Â      â”œâ”€â”€ calib
â”‚Â Â      â””â”€â”€ velodyne
â”œâ”€â”€ radar_3frames
â”‚Â Â  â”œâ”€â”€ testing
â”‚Â Â  â””â”€â”€ training
â”‚Â Â      â””â”€â”€ velodyne
â””â”€â”€ radar_5frames
    â”œâ”€â”€ testing
    â””â”€â”€ training
        â””â”€â”€ velodyne
```


## ğŸš€ Getting Started

Please ensure you running with an Ubuntu machine with Nvidia GPU (at least 2GB VRAM). 
The code is tested with Ubuntu22.04, and CUDA 11.8 with RTX 4090. Any other machine is not guaranteed to work.

To start, please ensure you have miniconda installed by following the official instructions [here](https://docs.anaconda.com/free/miniconda/miniconda-install/). 

First, clone the repository with the following command and navigate to the root directory of the project:

```bash
git clone git@github.com:LJacksonPan/RaTrack.git
cd RaTrack
```
Create a **RaTrack** environment with the following command:

```bash
conda env create -f environment.yml
```

This will setup a conda environment named `RaTrack` with CUDA 11.8, PyTorch2.2.0.

Installing the pointnet2 pytorch dependencies:

```bash
cd lib
python setup.py install
```

To train the model, please run:

```bash
python main.py
```
This will use the configuration file `config.yaml` to train the model.

To evaluate the model and generate the model predictions, please run:

```bash
python main.py --config configs_eval.yaml
```

## ğŸ” Evaluation

To evaluate with the trained RaTrack model, please open the `configs_eval.yaml` and change the `model_path` to the path of the trained model. 
```yaml
model_path: 'checkpoint/track4d_radar/models/model.last.t7'
```


Then run the following command:

```bash
python main.py --config configs_eval.yaml
```

This will generate the predictions in the `results` folder.

If you are interested in evaluating the predictions with our version of AB3DMOT evaluation, please contact us.

## ğŸ‘ Acknowledgements

We use the following open-source projects in our work:

- [Pointnet2.Pytorch](https://github.com/sshaoshuai/Pointnet2.PyTorch): We use the pytorch cuda implmentation of pointnet2 module.
- [view-of-delft-dataset](https://github.com/tudelft-iv/view-of-delft-dataset): We the documentation and development kit of the View of Delft (VoD) dataset to develop the model.
- [AB3DMOT](https://github.com/open-mmlab/OpenPCDet): we use AB3DMOT for evaluation metrics.
- [OpenPCDet](https://github.com/open-mmlab/OpenPCDet): we use OpenPCDet for baseline detetion model training and evaluating.